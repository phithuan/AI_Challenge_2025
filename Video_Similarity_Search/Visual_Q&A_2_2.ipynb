{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6aeffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from PIL import Image\n",
    "from pymilvus import MilvusClient\n",
    "import torch\n",
    "from transformers import AutoProcessor as HF_AutoProcessor, BlipForQuestionAnswering\n",
    "\n",
    "# ================== Config ==================\n",
    "DB_PARAMS = {\n",
    "    \"dbname\": \"video_frame\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "\n",
    "CSV_DIR   = r\"D:\\Big_project_2025\\Video_Similarity_Search\\data\\csv\"\n",
    "FRAME_DIR = r\"D:\\Big_project_2025\\Video_Similarity_Search\\data\\key_frame\"\n",
    "VIDEO_DIR = r\"D:\\Big_project_2025\\Video_Similarity_Search\\data\\video\"\n",
    "MODEL_DIR = r\"D:\\Big_project_2025\\huggingface_cache\"\n",
    "\n",
    "# ================== K·∫øt n·ªëi DB + Milvus ==================\n",
    "conn = psycopg2.connect(**DB_PARAMS)\n",
    "cur = conn.cursor()\n",
    "client = MilvusClient(uri=\"http://localhost:19530\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86fdb563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hhmmss_to_seconds(s: str) -> float:\n",
    "    s = s.strip().lower()\n",
    "    if \":\" in s:\n",
    "        parts = [float(x) for x in s.split(\":\")]\n",
    "        if len(parts) == 2: return parts[0]*60 + parts[1]\n",
    "        elif len(parts) == 3: return parts[0]*3600 + parts[1]*60 + parts[2]\n",
    "    m = re.search(r\"(\\d+(?:\\.\\d+)?)\\s*(s|gi√¢y|sec)\", s)\n",
    "    if m: return float(m.group(1))\n",
    "    return None\n",
    "\n",
    "def extract_time_from_question(q: str):\n",
    "    q_low = q.lower()\n",
    "    m_ts = re.search(r\"(\\d{1,2}:\\d{2}(:\\d{2})?)\", q_low)\n",
    "    if m_ts: return hhmmss_to_seconds(m_ts.group(1))\n",
    "    m_s = re.search(r\"(\\d+(?:\\.\\d+)?)\\s*(s|gi√¢y|sec)\", q_low)\n",
    "    if m_s: return float(m_s.group(1))\n",
    "    return None\n",
    "\n",
    "def load_video_csv(video_name: str):\n",
    "    csv_path = os.path.join(CSV_DIR, f\"{video_name}.csv\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df\n",
    "\n",
    "def seconds_to_frame_idx(video_name: str, seconds_value: float) -> int:\n",
    "    df = load_video_csv(video_name).sort_values(\"pts_time\").reset_index(drop=True)\n",
    "    frame_idx = np.interp(seconds_value, df[\"pts_time\"], df[\"frame_idx\"])\n",
    "    return int(round(frame_idx))\n",
    "\n",
    "def frame_idx_to_seconds(video_name: str, frame_idx: int) -> float:\n",
    "    df = load_video_csv(video_name).sort_values(\"frame_idx\").reset_index(drop=True)\n",
    "    seconds_val = np.interp(frame_idx, df[\"frame_idx\"], df[\"pts_time\"])\n",
    "    return float(seconds_val)\n",
    "\n",
    "def get_frame_image_path(video_name: str, frame_idx: int):\n",
    "    target_sec = frame_idx_to_seconds(video_name, frame_idx)\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT fm.frame_path, ABS(fm.pts_time - %s) AS diff\n",
    "        FROM frame_mappings fm\n",
    "        JOIN videos v ON fm.video_id = v.id\n",
    "        WHERE v.video_path LIKE %s\n",
    "        ORDER BY diff ASC\n",
    "        LIMIT 1\n",
    "    \"\"\", (target_sec, f\"%{video_name}%\"))\n",
    "    row = cur.fetchone()\n",
    "    if row: return row[0]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70731b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëâ ƒêang ch·∫°y tr√™n: CPU\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "The paging file is too small for this operation to complete. (os error 1455)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m vqa_processor = AutoProcessor.from_pretrained(MODEL_NAME, cache_dir=MODEL_DIR)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Load model v·ªõi c·∫•u h√¨nh ti·∫øt ki·ªám RAM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m vqa_model = \u001b[43mBlipForQuestionAnswering\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# CPU n√™n ƒë·ªÉ float32\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# gi·∫£m chi·∫øm d·ª•ng b·ªô nh·ªõ\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m.to(device)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Load th√†nh c√¥ng BLIP-VQA-BASE (CPU mode)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# ========== H√†m ch·∫°y VQA ==========\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Big_project_2025\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:317\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    315\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Big_project_2025\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4953\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4945\u001b[39m is_from_file = pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4947\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4948\u001b[39m     is_safetensors_available()\n\u001b[32m   4949\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m is_from_file\n\u001b[32m   4950\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded\n\u001b[32m   4951\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m checkpoint_files[\u001b[32m0\u001b[39m].endswith(\u001b[33m\"\u001b[39m\u001b[33m.safetensors\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4952\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m4953\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   4954\u001b[39m         metadata = f.metadata()\n\u001b[32m   4956\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4957\u001b[39m         \u001b[38;5;66;03m# Assume it's a pytorch checkpoint (introduced for timm checkpoints)\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: The paging file is too small for this operation to complete. (os error 1455)"
     ]
    }
   ],
   "source": [
    "# ========== Cell 3: Load BLIP VQA Base (CPU t·ªëi ∆∞u) ==========\n",
    "import torch\n",
    "from transformers import BlipForQuestionAnswering, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "# V·ªõi m√°y b·∫°n th√¨ lu√¥n ch·∫°y CPU\n",
    "device = \"cpu\"\n",
    "print(f\"üëâ ƒêang ch·∫°y tr√™n: {device.upper()}\")\n",
    "\n",
    "MODEL_DIR = r\"D:\\Big_project_2025\\huggingface_cache\"\n",
    "MODEL_NAME = \"Salesforce/blip-vqa-base\"\n",
    "\n",
    "# Load processor\n",
    "vqa_processor = AutoProcessor.from_pretrained(MODEL_NAME, cache_dir=MODEL_DIR)\n",
    "\n",
    "# Load model v·ªõi c·∫•u h√¨nh ti·∫øt ki·ªám RAM\n",
    "vqa_model = BlipForQuestionAnswering.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir=MODEL_DIR,\n",
    "    torch_dtype=torch.float32,   # CPU n√™n ƒë·ªÉ float32\n",
    "    low_cpu_mem_usage=True       # gi·∫£m chi·∫øm d·ª•ng b·ªô nh·ªõ\n",
    ").to(device)\n",
    "\n",
    "print(\"‚úÖ Load th√†nh c√¥ng BLIP-VQA-BASE (CPU mode)\")\n",
    "\n",
    "# ========== H√†m ch·∫°y VQA ==========\n",
    "def run_vqa(image_path: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Th·ª±c hi·ªán Visual Question Answering:\n",
    "    - image_path: ƒë∆∞·ªùng d·∫´n t·ªõi ·∫£nh (frame)\n",
    "    - question: c√¢u h·ªèi\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = vqa_processor(image, question, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = vqa_model.generate(**inputs)\n",
    "        answer = vqa_processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7365cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
