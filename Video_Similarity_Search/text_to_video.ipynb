{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e8edd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf12e77f6c4d40f4a1ee8a8e2096ed71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Big_project_2025\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\huggingface_cache\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a37464fa1be47aa8850a7a23a7aaf27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956c71ca8a414a91bfddf0660bce223c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f05e0158dd4b54932b3a720f2c3251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddf0b05176a473093c57199a9ab9eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fffaf0d5afa412cb03cc2ebad16398e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae61275e3ae4476daf4ac335e5ef1ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a57eb7954874dc9bbe4da518f24156d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc281296f0c45f4950c9177fdcb10cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số khung hình được xử lý: 327\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Tải mô hình CLIP một lần để tránh tải lặp lại\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def encode_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            image_features = model.get_image_features(**inputs)\n",
    "        return image_features[0].numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi xử lý {image_path}: {e}\")\n",
    "        return None  # Trả về None nếu lỗi, để lọc sau\n",
    "\n",
    "# Đọc danh sách khung hình từ thư mục\n",
    "frame_base_dir = \"data/key_frame\"\n",
    "video_base_dir = \"data/video\"\n",
    "frame_embeddings = []\n",
    "\n",
    "# Duyệt qua các thư mục con trong key_frame\n",
    "for key_frame_dir in os.listdir(frame_base_dir):\n",
    "    frame_dir_path = os.path.join(frame_base_dir, key_frame_dir)\n",
    "    if os.path.isdir(frame_dir_path):\n",
    "        # Lấy video_path từ tên thư mục (giả định khớp với tiền tố)\n",
    "        video_name = f\"{key_frame_dir}.mp4\"\n",
    "        video_path = os.path.join(video_base_dir, video_name).replace(\"\\\\\", \"/\")  # Chuẩn hóa đường dẫn\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"Video {video_path} không tồn tại, bỏ qua.\")\n",
    "            continue\n",
    "\n",
    "        # Lấy tất cả file khung hình trong thư mục\n",
    "        frame_paths = [os.path.join(frame_dir_path, f).replace(\"\\\\\", \"/\") for f in os.listdir(frame_dir_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        for frame_path in frame_paths:\n",
    "            embedding = encode_image(frame_path)\n",
    "            if embedding is not None:  # Chỉ thêm nếu thành công\n",
    "                frame_embeddings.append({\n",
    "                    \"video_path\": video_path,\n",
    "                    \"frame_path\": frame_path,\n",
    "                    \"vector\": embedding\n",
    "                })\n",
    "\n",
    "print(f\"Tổng số khung hình được xử lý: {len(frame_embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f099663f",
   "metadata": {},
   "source": [
    "# setup Lưu trữ Embeddings và Metadata trong PostgreSQL và Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993615e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, MilvusClient\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Kết nối PostgreSQL\n",
    "db_params = {\n",
    "    \"dbname\": \"video\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "conn = psycopg2.connect(**db_params)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Tạo bảng với ràng buộc duy nhất\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS videos (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        video_path VARCHAR(255) UNIQUE,\n",
    "        title VARCHAR(255),\n",
    "        description TEXT\n",
    "    )\n",
    "\"\"\")\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS frame_mappings (\n",
    "        frame_id SERIAL PRIMARY KEY,\n",
    "        video_id INTEGER REFERENCES videos(id),\n",
    "        frame_path VARCHAR(255),\n",
    "        milvus_id BIGINT\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee75d384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết nối thành công! Danh sách collection: ['video_search', 'Movies', 'image_collection', 'my_rag_collection']\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "# Kết nối tới Milvus server\n",
    "milvus_client = MilvusClient(uri=\"http://localhost:19530\")\n",
    "\n",
    "# Kiểm tra kết nối bằng cách liệt kê collection\n",
    "try:\n",
    "    collections = milvus_client.list_collections()\n",
    "    print(\"Kết nối thành công! Danh sách collection:\", collections)\n",
    "except Exception as e:\n",
    "    print(\"Kết nối thất bại:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de5e337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Big_project_2025\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at schema.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Big_project_2025\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at common.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Big_project_2025\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at milvus.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Big_project_2025\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at rg.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Big_project_2025\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at feder.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Big_project_2025\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at msg.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu đã được chèn và commit.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "collection_name = \"video_search\"\n",
    "if client.has_collection(collection_name):\n",
    "    client.drop_collection(collection_name)\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=512),\n",
    "    FieldSchema(name=\"frame_path\", dtype=DataType.VARCHAR, max_length=255)\n",
    "]\n",
    "schema = CollectionSchema(fields=fields, description=\"Embeddings khung hình video\")\n",
    "collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "index_params = {\"metric_type\": \"IP\", \"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 1024}}\n",
    "collection.create_index(field_name=\"vector\", index_params=index_params)\n",
    "collection.load()\n",
    "\n",
    "# Chèn dữ liệu\n",
    "frame_base_dir = \"data/key_frame\"\n",
    "video_base_dir = \"data/video\"\n",
    "for key_frame_dir in os.listdir(frame_base_dir):\n",
    "    frame_dir_path = os.path.join(frame_base_dir, key_frame_dir)\n",
    "    if os.path.isdir(frame_dir_path):\n",
    "        video_name = f\"{key_frame_dir}.mp4\"\n",
    "        video_path = os.path.join(video_base_dir, video_name).replace(\"\\\\\", \"/\")\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"Video {video_path} không tồn tại, bỏ qua.\")\n",
    "            continue\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO videos (video_path, title, description)\n",
    "            VALUES (%s, %s, %s) ON CONFLICT (video_path) DO NOTHING RETURNING id\n",
    "        \"\"\", (video_path, os.path.basename(video_path), \"Video mẫu\"))\n",
    "        video_id = cur.fetchone()\n",
    "        if video_id:\n",
    "            video_id = video_id[0]\n",
    "        else:\n",
    "            cur.execute(\"SELECT id FROM videos WHERE video_path = %s\", (video_path,))\n",
    "            video_id = cur.fetchone()[0]\n",
    "\n",
    "        frame_paths = [os.path.join(frame_dir_path, f).replace(\"\\\\\", \"/\") for f in os.listdir(frame_dir_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        for frame_path in frame_paths:\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO frame_mappings (video_id, frame_path)\n",
    "                VALUES (%s, %s) ON CONFLICT (frame_path) DO NOTHING RETURNING frame_id\n",
    "            \"\"\", (video_id, frame_path))\n",
    "            frame_id = cur.fetchone()\n",
    "\n",
    "            image = Image.open(frame_path)\n",
    "            inputs = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")(images=image, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                embedding = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").get_image_features(**inputs)[0].numpy()\n",
    "\n",
    "            data = {\"vector\": embedding, \"frame_path\": frame_path}\n",
    "            mr = collection.insert([data])\n",
    "            milvus_id = mr.primary_keys[0]\n",
    "            cur.execute(\"\"\"\n",
    "                UPDATE frame_mappings\n",
    "                SET milvus_id = %s\n",
    "                WHERE frame_path = %s\n",
    "            \"\"\", (milvus_id, frame_path))\n",
    "\n",
    "conn.commit()\n",
    "print(\"Dữ liệu đã được chèn và commit.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32ef349",
   "metadata": {},
   "source": [
    "# Tìm kiếm Image-to-Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed2c289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết nối Milvus thành công!\n",
      "Sử dụng thiết bị: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import psycopg2\n",
    "from pymilvus import connections, MilvusClient\n",
    "\n",
    "# Kết nối PostgreSQL\n",
    "db_params = {\n",
    "    \"dbname\": \"video\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "conn = psycopg2.connect(**db_params)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Kết nối Milvus\n",
    "try:\n",
    "    connections.connect(host=\"localhost\", port=\"19530\")\n",
    "    print(\"Kết nối Milvus thành công!\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi kết nối Milvus: {e}\")\n",
    "    exit()\n",
    "\n",
    "client = MilvusClient(uri=\"http://localhost:19530\")\n",
    "\n",
    "# Tải mô hình CLIP\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Sử dụng thiết bị: {device}\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Hàm tạo embedding từ hình ảnh\n",
    "def encode_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            image_features = model.get_image_features(**inputs)\n",
    "        return image_features[0].cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi xử lý {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Hàm tìm kiếm video\n",
    "def search_videos_by_image(image_path, top_k=10):\n",
    "    query_embedding = encode_image(image_path)\n",
    "    if query_embedding is None:\n",
    "        print(\"Không thể tạo embedding cho hình ảnh.\")\n",
    "        return []\n",
    "    search_results = client.search(\n",
    "        collection_name=\"video_search\",\n",
    "        data=[query_embedding],\n",
    "        limit=top_k,\n",
    "        output_fields=[\"frame_path\"]\n",
    "    )\n",
    "    return search_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb816b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: L21_V001.mp4, Đường dẫn: data/video/L21_V001.mp4, Độ tương đồng: 111.22276306152344\n",
      "Video: L21_V001.mp4, Đường dẫn: data/video/L21_V001.mp4, Độ tương đồng: 102.9091796875\n",
      "Video: L21_V002.mp4, Đường dẫn: data/video/L21_V002.mp4, Độ tương đồng: 85.33064270019531\n",
      "Video: L21_V001.mp4, Đường dẫn: data/video/L21_V001.mp4, Độ tương đồng: 83.57435607910156\n",
      "Video: L21_V001.mp4, Đường dẫn: data/video/L21_V001.mp4, Độ tương đồng: 81.2811050415039\n",
      "Video: L21_V001.mp4, Đường dẫn: data/video/L21_V001.mp4, Độ tương đồng: 80.79733276367188\n",
      "Video: L21_V002.mp4, Đường dẫn: data/video/L21_V002.mp4, Độ tương đồng: 80.4063949584961\n",
      "Video: L21_V001.mp4, Đường dẫn: data/video/L21_V001.mp4, Độ tương đồng: 80.2515640258789\n",
      "Video: L21_V001.mp4, Đường dẫn: data/video/L21_V001.mp4, Độ tương đồng: 79.47163391113281\n",
      "Video: L21_V001.mp4, Đường dẫn: data/video/L21_V001.mp4, Độ tương đồng: 78.57772827148438\n"
     ]
    }
   ],
   "source": [
    "# Thực hiện tìm kiếm\n",
    "image_path = \"data/key_frame/L21_V001/015.jpg\"  # Thay bằng đường dẫn ảnh bạn muốn query\n",
    "results = search_videos_by_image(image_path)\n",
    "for result in results[0]:\n",
    "    frame_path = result[\"entity\"][\"frame_path\"]\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT v.video_path, v.title\n",
    "        FROM frame_mappings fm\n",
    "        JOIN videos v ON fm.video_id = v.id\n",
    "        WHERE fm.frame_path = %s\n",
    "    \"\"\", (frame_path,))\n",
    "    video_info = cur.fetchone()\n",
    "    if video_info:\n",
    "        print(f\"Video: {video_info[1]}, Đường dẫn: {video_info[0]}, Độ tương đồng: {result['distance']}\")\n",
    "    else:\n",
    "        print(f\"Không tìm thấy video cho frame_path: {frame_path}\")\n",
    "\n",
    "# Đóng kết nối\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff82a38",
   "metadata": {},
   "source": [
    "# text to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fb91628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết nối Milvus thành công!\n",
      "Sử dụng thiết bị: cpu\n",
      "Video: L21_V002.mp4, Đường dẫn: data/video/L21_V002.mp4, Độ tương đồng: 3.0837035179138184\n",
      "Video: L21_V002.mp4, Đường dẫn: data/video/L21_V002.mp4, Độ tương đồng: 3.019639015197754\n",
      "Video: L21_V002.mp4, Đường dẫn: data/video/L21_V002.mp4, Độ tương đồng: 2.9320945739746094\n",
      "Video: L21_V002.mp4, Đường dẫn: data/video/L21_V002.mp4, Độ tương đồng: 2.86421537399292\n",
      "Video: L21_V002.mp4, Đường dẫn: data/video/L21_V002.mp4, Độ tương đồng: 2.8333077430725098\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import psycopg2\n",
    "from pymilvus import connections, MilvusClient\n",
    "\n",
    "# Sử dụng context manager cho kết nối PostgreSQL\n",
    "with psycopg2.connect(\n",
    "    dbname=\"video\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ") as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        # Kết nối Milvus\n",
    "        try:\n",
    "            connections.connect(host=\"localhost\", port=\"19530\")\n",
    "            print(\"Kết nối Milvus thành công!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi kết nối Milvus: {e}\")\n",
    "            exit()\n",
    "\n",
    "        client = MilvusClient(uri=\"http://localhost:19530\")\n",
    "\n",
    "        # Tải mô hình CLIP\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Sử dụng thiết bị: {device}\")\n",
    "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "        # Hàm tạo embedding từ text\n",
    "        def encode_text(text):\n",
    "            try:\n",
    "                inputs = processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True, max_length=77).to(device)\n",
    "                with torch.no_grad():\n",
    "                    text_features = model.get_text_features(**inputs)\n",
    "                # Chuẩn hóa embedding\n",
    "                normalized_features = F.normalize(text_features, p=2, dim=1)[0].cpu().numpy()\n",
    "                return normalized_features\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi khi xử lý text: {e}\")\n",
    "                return None\n",
    "\n",
    "        # Hàm tìm kiếm video dựa trên text\n",
    "        def search_videos_by_text(text_query, top_k=10):\n",
    "            text_embedding = encode_text(text_query)\n",
    "            if text_embedding is None:\n",
    "                print(\"Không thể tạo embedding cho text.\")\n",
    "                return []\n",
    "            search_results = client.search(\n",
    "                collection_name=\"video_search\",\n",
    "                data=[text_embedding],\n",
    "                limit=top_k,\n",
    "                output_fields=[\"frame_path\"]\n",
    "            )\n",
    "            return search_results\n",
    "\n",
    "        # Hàm lấy video từ frame_path\n",
    "        def get_video_from_frame(frame_path):\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT v.video_path, v.title\n",
    "                FROM frame_mappings fm\n",
    "                JOIN videos v ON fm.video_id = v.id\n",
    "                WHERE fm.frame_path = %s\n",
    "            \"\"\", (frame_path,))\n",
    "            video_info = cur.fetchone()\n",
    "            return video_info\n",
    "\n",
    "        # Thực hiện tìm kiếm dựa trên text\n",
    "        text_query = \"Pomelo tree\"  # Thay bằng mô tả bạn muốn\n",
    "        results = search_videos_by_text(text_query, top_k=5)  # Lấy 5 kết quả tốt nhất\n",
    "        if results:\n",
    "            for result in results[0]:\n",
    "                frame_path = result[\"entity\"][\"frame_path\"]\n",
    "                video_info = get_video_from_frame(frame_path)\n",
    "                if video_info:\n",
    "                    print(f\"Video: {video_info[1]}, Đường dẫn: {video_info[0]}, Độ tương đồng: {result['distance']}\")\n",
    "                else:\n",
    "                    print(f\"Không tìm thấy video cho frame_path: {frame_path}\")\n",
    "        else:\n",
    "            print(\"Không tìm thấy kết quả nào.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d27e07e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 KẾT QUẢ CHI TIẾT TỪ MILVUS\n",
      "🎬 Frame: data/key_frame/L21_V002/077.jpg | ⏱ 279.13s (4:39) | 🔍 Độ tương đồng: 3.0837\n",
      "🎬 Frame: data/key_frame/L21_V002/067.jpg | ⏱ 238.07s (3:58) | 🔍 Độ tương đồng: 3.0196\n",
      "🎬 Frame: data/key_frame/L21_V002/070.jpg | ⏱ 254.33s (4:14) | 🔍 Độ tương đồng: 2.9321\n",
      "🎬 Frame: data/key_frame/L21_V002/079.jpg | ⏱ 283.57s (4:43) | 🔍 Độ tương đồng: 2.8642\n",
      "🎬 Frame: data/key_frame/L21_V002/068.jpg | ⏱ 242.57s (4:02) | 🔍 Độ tương đồng: 2.8333\n",
      "\n",
      "📌 KHOẢNG THỜI GIAN GOM LẠI\n",
      "👉 Xuất hiện từ 3:58 (238.07s) đến 4:14 (254.33s)\n",
      "👉 Xuất hiện từ 4:39 (279.13s) đến 4:43 (283.57s)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load CSV mapping (có cột: n, pts_time, fps, frame_idx)\n",
    "mapping_df = pd.read_csv(r\"D:\\Big_project_2025\\Video_Similarity_Search\\data\\L21_V002.csv\")\n",
    "\n",
    "def get_time_from_keyframe(frame_path):\n",
    "    \"\"\"Lấy pts_time từ tên keyframe.\"\"\"\n",
    "    filename = os.path.splitext(os.path.basename(frame_path))[0]  # \"067\"\n",
    "    try:\n",
    "        n = int(filename)  # dùng cột n trong CSV\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "    row = mapping_df[mapping_df[\"n\"] == n]\n",
    "    if not row.empty:\n",
    "        return float(row[\"pts_time\"].values[0])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def group_timestamps(timestamps, gap_threshold=10.0):\n",
    "    \"\"\"Gom nhiều timestamp gần nhau thành khoảng [start, end].\"\"\"\n",
    "    if not timestamps:\n",
    "        return []\n",
    "    timestamps = sorted(timestamps)\n",
    "    ranges = []\n",
    "    start = timestamps[0]\n",
    "    end = timestamps[0]\n",
    "\n",
    "    for t in timestamps[1:]:\n",
    "        if t - end <= gap_threshold:\n",
    "            end = t\n",
    "        else:\n",
    "            ranges.append((start, end))\n",
    "            start = t\n",
    "            end = t\n",
    "    ranges.append((start, end))\n",
    "    return ranges\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Chuyển giây -> phút:giây (MM:SS).\"\"\"\n",
    "    m = int(seconds // 60)\n",
    "    s = int(seconds % 60)\n",
    "    return f\"{m}:{s:02d}\"\n",
    "\n",
    "# ================== Demo ==================\n",
    "timestamps = []\n",
    "\n",
    "print(\"📌 KẾT QUẢ CHI TIẾT TỪ MILVUS\")\n",
    "for result in results[0]:\n",
    "    frame_path = result[\"entity\"][\"frame_path\"]\n",
    "    ts = get_time_from_keyframe(frame_path)\n",
    "    if ts is not None:\n",
    "        distance = result['distance']\n",
    "        print(f\"🎬 Frame: {frame_path} | ⏱ {ts:.2f}s ({format_time(ts)}) | 🔍 Độ tương đồng: {distance:.4f}\")\n",
    "        timestamps.append(ts)\n",
    "\n",
    "# Gom các frame gần nhau thành khoảng\n",
    "time_ranges = group_timestamps(timestamps, gap_threshold=15.0)\n",
    "\n",
    "print(\"\\n📌 KHOẢNG THỜI GIAN GOM LẠI\")\n",
    "for start, end in time_ranges:\n",
    "    print(f\"👉 Xuất hiện từ {format_time(start)} ({start:.2f}s) đến {format_time(end)} ({end:.2f}s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64c127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
