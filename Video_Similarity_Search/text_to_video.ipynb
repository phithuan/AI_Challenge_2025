{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e8edd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf12e77f6c4d40f4a1ee8a8e2096ed71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Big_project_2025\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\huggingface_cache\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a37464fa1be47aa8850a7a23a7aaf27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956c71ca8a414a91bfddf0660bce223c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f05e0158dd4b54932b3a720f2c3251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddf0b05176a473093c57199a9ab9eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fffaf0d5afa412cb03cc2ebad16398e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae61275e3ae4476daf4ac335e5ef1ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a57eb7954874dc9bbe4da518f24156d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc281296f0c45f4950c9177fdcb10cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng s·ªë khung h√¨nh ƒë∆∞·ª£c x·ª≠ l√Ω: 327\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# T·∫£i m√¥ h√¨nh CLIP m·ªôt l·∫ßn ƒë·ªÉ tr√°nh t·∫£i l·∫∑p l·∫°i\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def encode_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            image_features = model.get_image_features(**inputs)\n",
    "        return image_features[0].numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói khi x·ª≠ l√Ω {image_path}: {e}\")\n",
    "        return None  # Tr·∫£ v·ªÅ None n·∫øu l·ªói, ƒë·ªÉ l·ªçc sau\n",
    "\n",
    "# ƒê·ªçc danh s√°ch khung h√¨nh t·ª´ th∆∞ m·ª•c\n",
    "frame_base_dir = \"data/key_frame\"\n",
    "video_base_dir = \"data/video\"\n",
    "frame_embeddings = []\n",
    "\n",
    "# Duy·ªát qua c√°c th∆∞ m·ª•c con trong key_frame\n",
    "for key_frame_dir in os.listdir(frame_base_dir):\n",
    "    frame_dir_path = os.path.join(frame_base_dir, key_frame_dir)\n",
    "    if os.path.isdir(frame_dir_path):\n",
    "        # L·∫•y video_path t·ª´ t√™n th∆∞ m·ª•c (gi·∫£ ƒë·ªãnh kh·ªõp v·ªõi ti·ªÅn t·ªë)\n",
    "        video_name = f\"{key_frame_dir}.mp4\"\n",
    "        video_path = os.path.join(video_base_dir, video_name).replace(\"\\\\\", \"/\")  # Chu·∫©n h√≥a ƒë∆∞·ªùng d·∫´n\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"Video {video_path} kh√¥ng t·ªìn t·∫°i, b·ªè qua.\")\n",
    "            continue\n",
    "\n",
    "        # L·∫•y t·∫•t c·∫£ file khung h√¨nh trong th∆∞ m·ª•c\n",
    "        frame_paths = [os.path.join(frame_dir_path, f).replace(\"\\\\\", \"/\") for f in os.listdir(frame_dir_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        for frame_path in frame_paths:\n",
    "            embedding = encode_image(frame_path)\n",
    "            if embedding is not None:  # Ch·ªâ th√™m n·∫øu th√†nh c√¥ng\n",
    "                frame_embeddings.append({\n",
    "                    \"video_path\": video_path,\n",
    "                    \"frame_path\": frame_path,\n",
    "                    \"vector\": embedding\n",
    "                })\n",
    "\n",
    "print(f\"T·ªïng s·ªë khung h√¨nh ƒë∆∞·ª£c x·ª≠ l√Ω: {len(frame_embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f099663f",
   "metadata": {},
   "source": [
    "# setup L∆∞u tr·ªØ Embeddings v√† Metadata trong PostgreSQL v√† Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993615e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, MilvusClient\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# K·∫øt n·ªëi PostgreSQL\n",
    "db_params = {\n",
    "    \"dbname\": \"video\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "conn = psycopg2.connect(**db_params)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# T·∫°o b·∫£ng v·ªõi r√†ng bu·ªôc duy nh·∫•t\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS videos (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        video_path VARCHAR(255) UNIQUE,\n",
    "        title VARCHAR(255),\n",
    "        description TEXT\n",
    "    )\n",
    "\"\"\")\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS frame_mappings (\n",
    "        frame_id SERIAL PRIMARY KEY,\n",
    "        video_id INTEGER REFERENCES videos(id),\n",
    "        frame_path VARCHAR(255),\n",
    "        milvus_id BIGINT\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee75d384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K·∫øt n·ªëi th√†nh c√¥ng! Danh s√°ch collection: ['video_search', 'Movies', 'image_collection', 'my_rag_collection']\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "# K·∫øt n·ªëi t·ªõi Milvus server\n",
    "milvus_client = MilvusClient(uri=\"http://localhost:19530\")\n",
    "\n",
    "# Ki·ªÉm tra k·∫øt n·ªëi b·∫±ng c√°ch li·ªát k√™ collection\n",
    "try:\n",
    "    collections = milvus_client.list_collections()\n",
    "    print(\"K·∫øt n·ªëi th√†nh c√¥ng! Danh s√°ch collection:\", collections)\n",
    "except Exception as e:\n",
    "    print(\"K·∫øt n·ªëi th·∫•t b·∫°i:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de5e337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Big_project_2025\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at schema.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Big_project_2025\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at common.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Big_project_2025\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at milvus.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Big_project_2025\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at rg.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Big_project_2025\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at feder.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Big_project_2025\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at msg.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c ch√®n v√† commit.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "collection_name = \"video_search\"\n",
    "if client.has_collection(collection_name):\n",
    "    client.drop_collection(collection_name)\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=512),\n",
    "    FieldSchema(name=\"frame_path\", dtype=DataType.VARCHAR, max_length=255)\n",
    "]\n",
    "schema = CollectionSchema(fields=fields, description=\"Embeddings khung h√¨nh video\")\n",
    "collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "index_params = {\"metric_type\": \"IP\", \"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 1024}}\n",
    "collection.create_index(field_name=\"vector\", index_params=index_params)\n",
    "collection.load()\n",
    "\n",
    "# Ch√®n d·ªØ li·ªáu\n",
    "frame_base_dir = \"data/key_frame\"\n",
    "video_base_dir = \"data/video\"\n",
    "for key_frame_dir in os.listdir(frame_base_dir):\n",
    "    frame_dir_path = os.path.join(frame_base_dir, key_frame_dir)\n",
    "    if os.path.isdir(frame_dir_path):\n",
    "        video_name = f\"{key_frame_dir}.mp4\"\n",
    "        video_path = os.path.join(video_base_dir, video_name).replace(\"\\\\\", \"/\")\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"Video {video_path} kh√¥ng t·ªìn t·∫°i, b·ªè qua.\")\n",
    "            continue\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO videos (video_path, title, description)\n",
    "            VALUES (%s, %s, %s) ON CONFLICT (video_path) DO NOTHING RETURNING id\n",
    "        \"\"\", (video_path, os.path.basename(video_path), \"Video m·∫´u\"))\n",
    "        video_id = cur.fetchone()\n",
    "        if video_id:\n",
    "            video_id = video_id[0]\n",
    "        else:\n",
    "            cur.execute(\"SELECT id FROM videos WHERE video_path = %s\", (video_path,))\n",
    "            video_id = cur.fetchone()[0]\n",
    "\n",
    "        frame_paths = [os.path.join(frame_dir_path, f).replace(\"\\\\\", \"/\") for f in os.listdir(frame_dir_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        for frame_path in frame_paths:\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO frame_mappings (video_id, frame_path)\n",
    "                VALUES (%s, %s) ON CONFLICT (frame_path) DO NOTHING RETURNING frame_id\n",
    "            \"\"\", (video_id, frame_path))\n",
    "            frame_id = cur.fetchone()\n",
    "\n",
    "            image = Image.open(frame_path)\n",
    "            inputs = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")(images=image, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                embedding = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").get_image_features(**inputs)[0].numpy()\n",
    "\n",
    "            data = {\"vector\": embedding, \"frame_path\": frame_path}\n",
    "            mr = collection.insert([data])\n",
    "            milvus_id = mr.primary_keys[0]\n",
    "            cur.execute(\"\"\"\n",
    "                UPDATE frame_mappings\n",
    "                SET milvus_id = %s\n",
    "                WHERE frame_path = %s\n",
    "            \"\"\", (milvus_id, frame_path))\n",
    "\n",
    "conn.commit()\n",
    "print(\"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c ch√®n v√† commit.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32ef349",
   "metadata": {},
   "source": [
    "# T√¨m ki·∫øm Image-to-Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed2c289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K·∫øt n·ªëi Milvus th√†nh c√¥ng!\n",
      "S·ª≠ d·ª•ng thi·∫øt b·ªã: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import psycopg2\n",
    "from pymilvus import connections, MilvusClient\n",
    "\n",
    "# K·∫øt n·ªëi PostgreSQL\n",
    "db_params = {\n",
    "    \"dbname\": \"video\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "conn = psycopg2.connect(**db_params)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# K·∫øt n·ªëi Milvus\n",
    "try:\n",
    "    connections.connect(host=\"localhost\", port=\"19530\")\n",
    "    print(\"K·∫øt n·ªëi Milvus th√†nh c√¥ng!\")\n",
    "except Exception as e:\n",
    "    print(f\"L·ªói k·∫øt n·ªëi Milvus: {e}\")\n",
    "    exit()\n",
    "\n",
    "client = MilvusClient(uri=\"http://localhost:19530\")\n",
    "\n",
    "# T·∫£i m√¥ h√¨nh CLIP\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device}\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# H√†m t·∫°o embedding t·ª´ h√¨nh ·∫£nh\n",
    "def encode_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            image_features = model.get_image_features(**inputs)\n",
    "        return image_features[0].cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói khi x·ª≠ l√Ω {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# H√†m t√¨m ki·∫øm video\n",
    "def search_videos_by_image(image_path, top_k=10):\n",
    "    query_embedding = encode_image(image_path)\n",
    "    if query_embedding is None:\n",
    "        print(\"Kh√¥ng th·ªÉ t·∫°o embedding cho h√¨nh ·∫£nh.\")\n",
    "        return []\n",
    "    search_results = client.search(\n",
    "        collection_name=\"video_search\",\n",
    "        data=[query_embedding],\n",
    "        limit=top_k,\n",
    "        output_fields=[\"frame_path\"]\n",
    "    )\n",
    "    return search_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb816b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: L21_V001.mp4, ƒê∆∞·ªùng d·∫´n: data/video/L21_V001.mp4, ƒê·ªô t∆∞∆°ng ƒë·ªìng: 111.22276306152344\n",
      "Video: L21_V001.mp4, ƒê∆∞·ªùng d·∫´n: data/video/L21_V001.mp4, ƒê·ªô t∆∞∆°ng ƒë·ªìng: 102.9091796875\n",
      "Video: L21_V002.mp4, ƒê∆∞·ªùng d·∫´n: data/video/L21_V002.mp4, ƒê·ªô t∆∞∆°ng ƒë·ªìng: 85.33064270019531\n",
      "Video: L21_V001.mp4, ƒê∆∞·ªùng d·∫´n: data/video/L21_V001.mp4, ƒê·ªô t∆∞∆°ng ƒë·ªìng: 83.57435607910156\n",
      "Video: L21_V001.mp4, ƒê∆∞·ªùng d·∫´n: data/video/L21_V001.mp4, ƒê·ªô t∆∞∆°ng ƒë·ªìng: 81.2811050415039\n",
      "Video: L21_V001.mp4, ƒê∆∞·ªùng d·∫´n: data/video/L21_V001.mp4, ƒê·ªô t∆∞∆°ng ƒë·ªìng: 80.79733276367188\n",
      "Video: L21_V002.mp4, ƒê∆∞·ªùng d·∫´n: data/video/L21_V002.mp4, ƒê·ªô t∆∞∆°ng ƒë·ªìng: 80.4063949584961\n",
      "Video: L21_V001.mp4, ƒê∆∞·ªùng d·∫´n: data/video/L21_V001.mp4, ƒê·ªô t∆∞∆°ng ƒë·ªìng: 80.2515640258789\n",
      "Video: L21_V001.mp4, ƒê∆∞·ªùng d·∫´n: data/video/L21_V001.mp4, ƒê·ªô t∆∞∆°ng ƒë·ªìng: 79.47163391113281\n",
      "Video: L21_V001.mp4, ƒê∆∞·ªùng d·∫´n: data/video/L21_V001.mp4, ƒê·ªô t∆∞∆°ng ƒë·ªìng: 78.57772827148438\n"
     ]
    }
   ],
   "source": [
    "# Th·ª±c hi·ªán t√¨m ki·∫øm\n",
    "image_path = \"data/key_frame/L21_V001/015.jpg\"  # Thay b·∫±ng ƒë∆∞·ªùng d·∫´n ·∫£nh b·∫°n mu·ªën query\n",
    "results = search_videos_by_image(image_path)\n",
    "for result in results[0]:\n",
    "    frame_path = result[\"entity\"][\"frame_path\"]\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT v.video_path, v.title\n",
    "        FROM frame_mappings fm\n",
    "        JOIN videos v ON fm.video_id = v.id\n",
    "        WHERE fm.frame_path = %s\n",
    "    \"\"\", (frame_path,))\n",
    "    video_info = cur.fetchone()\n",
    "    if video_info:\n",
    "        print(f\"Video: {video_info[1]}, ƒê∆∞·ªùng d·∫´n: {video_info[0]}, ƒê·ªô t∆∞∆°ng ƒë·ªìng: {result['distance']}\")\n",
    "    else:\n",
    "        print(f\"Kh√¥ng t√¨m th·∫•y video cho frame_path: {frame_path}\")\n",
    "\n",
    "# ƒê√≥ng k·∫øt n·ªëi\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff82a38",
   "metadata": {},
   "source": [
    "# text to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fb91628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K·∫øt n·ªëi Milvus th√†nh c√¥ng!\n",
      "S·ª≠ d·ª•ng thi·∫øt b·ªã: cpu\n",
      "Video: L21_V002.mp4, ƒê∆∞·ªùng d·∫´n: data/video/L21_V002.mp4, ƒê·ªô t∆∞∆°ng ƒë·ªìng: 3.0837035179138184\n",
      "Video: L21_V002.mp4, ƒê∆∞·ªùng d·∫´n: data/video/L21_V002.mp4, ƒê·ªô t∆∞∆°ng ƒë·ªìng: 3.019639015197754\n",
      "Video: L21_V002.mp4, ƒê∆∞·ªùng d·∫´n: data/video/L21_V002.mp4, ƒê·ªô t∆∞∆°ng ƒë·ªìng: 2.9320945739746094\n",
      "Video: L21_V002.mp4, ƒê∆∞·ªùng d·∫´n: data/video/L21_V002.mp4, ƒê·ªô t∆∞∆°ng ƒë·ªìng: 2.86421537399292\n",
      "Video: L21_V002.mp4, ƒê∆∞·ªùng d·∫´n: data/video/L21_V002.mp4, ƒê·ªô t∆∞∆°ng ƒë·ªìng: 2.8333077430725098\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import psycopg2\n",
    "from pymilvus import connections, MilvusClient\n",
    "\n",
    "# S·ª≠ d·ª•ng context manager cho k·∫øt n·ªëi PostgreSQL\n",
    "with psycopg2.connect(\n",
    "    dbname=\"video\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ") as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        # K·∫øt n·ªëi Milvus\n",
    "        try:\n",
    "            connections.connect(host=\"localhost\", port=\"19530\")\n",
    "            print(\"K·∫øt n·ªëi Milvus th√†nh c√¥ng!\")\n",
    "        except Exception as e:\n",
    "            print(f\"L·ªói k·∫øt n·ªëi Milvus: {e}\")\n",
    "            exit()\n",
    "\n",
    "        client = MilvusClient(uri=\"http://localhost:19530\")\n",
    "\n",
    "        # T·∫£i m√¥ h√¨nh CLIP\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device}\")\n",
    "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "        # H√†m t·∫°o embedding t·ª´ text\n",
    "        def encode_text(text):\n",
    "            try:\n",
    "                inputs = processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True, max_length=77).to(device)\n",
    "                with torch.no_grad():\n",
    "                    text_features = model.get_text_features(**inputs)\n",
    "                # Chu·∫©n h√≥a embedding\n",
    "                normalized_features = F.normalize(text_features, p=2, dim=1)[0].cpu().numpy()\n",
    "                return normalized_features\n",
    "            except Exception as e:\n",
    "                print(f\"L·ªói khi x·ª≠ l√Ω text: {e}\")\n",
    "                return None\n",
    "\n",
    "        # H√†m t√¨m ki·∫øm video d·ª±a tr√™n text\n",
    "        def search_videos_by_text(text_query, top_k=10):\n",
    "            text_embedding = encode_text(text_query)\n",
    "            if text_embedding is None:\n",
    "                print(\"Kh√¥ng th·ªÉ t·∫°o embedding cho text.\")\n",
    "                return []\n",
    "            search_results = client.search(\n",
    "                collection_name=\"video_search\",\n",
    "                data=[text_embedding],\n",
    "                limit=top_k,\n",
    "                output_fields=[\"frame_path\"]\n",
    "            )\n",
    "            return search_results\n",
    "\n",
    "        # H√†m l·∫•y video t·ª´ frame_path\n",
    "        def get_video_from_frame(frame_path):\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT v.video_path, v.title\n",
    "                FROM frame_mappings fm\n",
    "                JOIN videos v ON fm.video_id = v.id\n",
    "                WHERE fm.frame_path = %s\n",
    "            \"\"\", (frame_path,))\n",
    "            video_info = cur.fetchone()\n",
    "            return video_info\n",
    "\n",
    "        # Th·ª±c hi·ªán t√¨m ki·∫øm d·ª±a tr√™n text\n",
    "        text_query = \"Pomelo tree\"  # Thay b·∫±ng m√¥ t·∫£ b·∫°n mu·ªën\n",
    "        results = search_videos_by_text(text_query, top_k=5)  # L·∫•y 5 k·∫øt qu·∫£ t·ªët nh·∫•t\n",
    "        if results:\n",
    "            for result in results[0]:\n",
    "                frame_path = result[\"entity\"][\"frame_path\"]\n",
    "                video_info = get_video_from_frame(frame_path)\n",
    "                if video_info:\n",
    "                    print(f\"Video: {video_info[1]}, ƒê∆∞·ªùng d·∫´n: {video_info[0]}, ƒê·ªô t∆∞∆°ng ƒë·ªìng: {result['distance']}\")\n",
    "                else:\n",
    "                    print(f\"Kh√¥ng t√¨m th·∫•y video cho frame_path: {frame_path}\")\n",
    "        else:\n",
    "            print(\"Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d27e07e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå K·∫æT QU·∫¢ CHI TI·∫æT T·ª™ MILVUS\n",
      "üé¨ Frame: data/key_frame/L21_V002/077.jpg | ‚è± 279.13s (4:39) | üîç ƒê·ªô t∆∞∆°ng ƒë·ªìng: 3.0837\n",
      "üé¨ Frame: data/key_frame/L21_V002/067.jpg | ‚è± 238.07s (3:58) | üîç ƒê·ªô t∆∞∆°ng ƒë·ªìng: 3.0196\n",
      "üé¨ Frame: data/key_frame/L21_V002/070.jpg | ‚è± 254.33s (4:14) | üîç ƒê·ªô t∆∞∆°ng ƒë·ªìng: 2.9321\n",
      "üé¨ Frame: data/key_frame/L21_V002/079.jpg | ‚è± 283.57s (4:43) | üîç ƒê·ªô t∆∞∆°ng ƒë·ªìng: 2.8642\n",
      "üé¨ Frame: data/key_frame/L21_V002/068.jpg | ‚è± 242.57s (4:02) | üîç ƒê·ªô t∆∞∆°ng ƒë·ªìng: 2.8333\n",
      "\n",
      "üìå KHO·∫¢NG TH·ªúI GIAN GOM L·∫†I\n",
      "üëâ Xu·∫•t hi·ªán t·ª´ 3:58 (238.07s) ƒë·∫øn 4:14 (254.33s)\n",
      "üëâ Xu·∫•t hi·ªán t·ª´ 4:39 (279.13s) ƒë·∫øn 4:43 (283.57s)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load CSV mapping (c√≥ c·ªôt: n, pts_time, fps, frame_idx)\n",
    "mapping_df = pd.read_csv(r\"D:\\Big_project_2025\\Video_Similarity_Search\\data\\L21_V002.csv\")\n",
    "\n",
    "def get_time_from_keyframe(frame_path):\n",
    "    \"\"\"L·∫•y pts_time t·ª´ t√™n keyframe.\"\"\"\n",
    "    filename = os.path.splitext(os.path.basename(frame_path))[0]  # \"067\"\n",
    "    try:\n",
    "        n = int(filename)  # d√πng c·ªôt n trong CSV\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "    row = mapping_df[mapping_df[\"n\"] == n]\n",
    "    if not row.empty:\n",
    "        return float(row[\"pts_time\"].values[0])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def group_timestamps(timestamps, gap_threshold=10.0):\n",
    "    \"\"\"Gom nhi·ªÅu timestamp g·∫ßn nhau th√†nh kho·∫£ng [start, end].\"\"\"\n",
    "    if not timestamps:\n",
    "        return []\n",
    "    timestamps = sorted(timestamps)\n",
    "    ranges = []\n",
    "    start = timestamps[0]\n",
    "    end = timestamps[0]\n",
    "\n",
    "    for t in timestamps[1:]:\n",
    "        if t - end <= gap_threshold:\n",
    "            end = t\n",
    "        else:\n",
    "            ranges.append((start, end))\n",
    "            start = t\n",
    "            end = t\n",
    "    ranges.append((start, end))\n",
    "    return ranges\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Chuy·ªÉn gi√¢y -> ph√∫t:gi√¢y (MM:SS).\"\"\"\n",
    "    m = int(seconds // 60)\n",
    "    s = int(seconds % 60)\n",
    "    return f\"{m}:{s:02d}\"\n",
    "\n",
    "# ================== Demo ==================\n",
    "timestamps = []\n",
    "\n",
    "print(\"üìå K·∫æT QU·∫¢ CHI TI·∫æT T·ª™ MILVUS\")\n",
    "for result in results[0]:\n",
    "    frame_path = result[\"entity\"][\"frame_path\"]\n",
    "    ts = get_time_from_keyframe(frame_path)\n",
    "    if ts is not None:\n",
    "        distance = result['distance']\n",
    "        print(f\"üé¨ Frame: {frame_path} | ‚è± {ts:.2f}s ({format_time(ts)}) | üîç ƒê·ªô t∆∞∆°ng ƒë·ªìng: {distance:.4f}\")\n",
    "        timestamps.append(ts)\n",
    "\n",
    "# Gom c√°c frame g·∫ßn nhau th√†nh kho·∫£ng\n",
    "time_ranges = group_timestamps(timestamps, gap_threshold=15.0)\n",
    "\n",
    "print(\"\\nüìå KHO·∫¢NG TH·ªúI GIAN GOM L·∫†I\")\n",
    "for start, end in time_ranges:\n",
    "    print(f\"üëâ Xu·∫•t hi·ªán t·ª´ {format_time(start)} ({start:.2f}s) ƒë·∫øn {format_time(end)} ({end:.2f}s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64c127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
